{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8H4sgtJBO9X",
        "outputId": "4681ff33-f988-46d2-b4a4-580733666bda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting imgaug\n",
            "  Downloading imgaug-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.5)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.11/dist-packages (from imgaug) (0.25.2)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from imgaug) (2.37.0)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from imgaug) (2.0.7)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.10.6)\n",
            "Requirement already satisfied: albucore==0.0.23 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.23)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.11.0.86)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (3.12.3)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (6.2.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14.2->imgaug) (3.4.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14.2->imgaug) (2025.3.13)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.14.2->imgaug) (0.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.0/948.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imgaug\n",
            "Successfully installed imgaug-0.4.0\n"
          ]
        }
      ],
      "source": [
        "# 📌 Install Required Libraries\n",
        "!pip install tensorflow numpy opencv-python matplotlib scikit-learn imgaug albumentations gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 📌 Import Libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gdown\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications import EfficientNetB3\n",
        "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Reshape, Conv2DTranspose, Lambda, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.covariance import EmpiricalCovariance\n",
        "import albumentations as A\n",
        "import random\n",
        "from scipy.ndimage import gaussian_filter, map_coordinates"
      ],
      "metadata": {
        "id": "iGApRJ0HBQ4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 📌 Load dataset paths\n",
        "train_normal_path = \"/content/drive/MyDrive/data/NORMAL\"\n",
        "test_normal_path = \"/content/drive/MyDrive/data/test/NORMAL\"\n",
        "test_pneumonia_path = \"/content/drive/MyDrive/data/test/PNEUMONIA\"\n",
        "IMG_SIZE = 300  # EfficientNetB3 input size"
      ],
      "metadata": {
        "id": "VYc-Ufj7BVjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 📌 Lung Segmentation + CLAHE Only\n",
        "def lung_segmentation(image):\n",
        "    \"\"\"Basic segmentation using thresholding and contour detection\"\"\"\n",
        "    # Threshold to binary image\n",
        "    _, thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Find contours and fill the largest 2 (lungs)\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    mask = np.zeros_like(image)\n",
        "    contours = sorted(contours, key=cv2.contourArea, reverse=True)[:2]\n",
        "    cv2.drawContours(mask, contours, -1, color=255, thickness=-1)\n",
        "\n",
        "    # Apply mask\n",
        "    segmented = cv2.bitwise_and(image, image, mask=mask)\n",
        "    return segmented\n",
        "\n",
        "def apply_preprocessing(image):\n",
        "    image = lung_segmentation(image)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    image = clahe.apply(image)\n",
        "    return image\n"
      ],
      "metadata": {
        "id": "1kj2l0DSBck5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 📌 Augmentation Functions\n",
        "augmentations = A.Compose([\n",
        "    A.Rotate(limit=10, p=0.5),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.5),\n",
        "    A.GaussianBlur(blur_limit=(3, 7), p=0.5),\n",
        "    A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.5),\n",
        "])"
      ],
      "metadata": {
        "id": "q4AkZrum2s1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ad0c21-b8a5-4894-e0ee-17cb6c626864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-d2613c4a22c6>:7: UserWarning: Argument(s) 'alpha_affine' are not valid for transform ElasticTransform\n",
            "  A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.5),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_augmentations(image):\n",
        "    augmented = augmentations(image=image)\n",
        "    return augmented[\"image\"]"
      ],
      "metadata": {
        "id": "ouAFQresCdxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 📌 Load Images with Preprocessing and Augmentation\n",
        "def load_images(folder, augment=False):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img_path = os.path.join(folder, filename)\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None:\n",
        "            continue\n",
        "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "        # Apply preprocessing\n",
        "        img = apply_preprocessing(img)\n",
        "\n",
        "        # Expand dims to add channel axis\n",
        "        img = np.expand_dims(img, axis=-1)\n",
        "\n",
        "        # Apply augmentations if enabled\n",
        "        if augment:\n",
        "            img = np.squeeze(img, axis=-1)  # Remove channel axis for augmentation\n",
        "            img = apply_augmentations(img)\n",
        "            img = np.expand_dims(img, axis=-1)  # Add channel axis back\n",
        "\n",
        "        images.append(img)\n",
        "\n",
        "    return np.array(images)"
      ],
      "metadata": {
        "id": "r4Vanqk655aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 📌 Load and preprocess datasets\n",
        "train_normal = load_images(train_normal_path, augment=True)\n",
        "test_normal = load_images(test_normal_path, augment=False)\n",
        "test_pneumonia = load_images(test_pneumonia_path, augment=False)"
      ],
      "metadata": {
        "id": "IoyeOOnwCoM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert grayscale to RGB for EfficientNetB3\n",
        "train_normal = np.repeat(train_normal, 3, axis=-1)\n",
        "test_normal = np.repeat(test_normal, 3, axis=-1)\n",
        "test_pneumonia = np.repeat(test_pneumonia, 3, axis=-1)"
      ],
      "metadata": {
        "id": "J_zvErziCrUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 📌 EfficientNetB3 Model (Feature Extractor)\n",
        "base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "for layer in base_model.layers[:-20]:\n",
        "    layer.trainable = False\n",
        "\n",
        "'''x = GlobalAveragePooling2D()(base_model.output)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "classifier_output = Dense(1, activation='sigmoid')(x)\n",
        "classifier_model = Model(base_model.input, classifier_output)\n",
        "classifier_model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 📌 Train classifier on pneumonia images\n",
        "classifier_model.fit(train_pneumonia, np.ones(len(train_pneumonia)), epochs=5, batch_size=16, validation_split=0.1)'''"
      ],
      "metadata": {
        "id": "fToAzh0GJJmV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "05ae1787-9458-4c2f-c762-686d85fc411e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n",
            "\u001b[1m43941136/43941136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"x = GlobalAveragePooling2D()(base_model.output)\\nx = Dense(256, activation='relu')(x)\\nx = Dropout(0.3)(x)\\nclassifier_output = Dense(1, activation='sigmoid')(x)\\nclassifier_model = Model(base_model.input, classifier_output)\\nclassifier_model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\\n\\n# 📌 Train classifier on pneumonia images\\nclassifier_model.fit(train_pneumonia, np.ones(len(train_pneumonia)), epochs=5, batch_size=16, validation_split=0.1)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 📌 VAE Model\n",
        "latent_dim = 64\n",
        "\n",
        "# Encoder\n",
        "inputs = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "x = base_model(inputs, training=False)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "z_mean = Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "z = Lambda(sampling, output_shape=(latent_dim,), name=\"z\")([z_mean, z_log_var])\n",
        "\n",
        "encoder = Model(inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "# Decoder\n",
        "decoder_input = Input(shape=(latent_dim,))\n",
        "x = Dense(75 * 75 * 64, activation='relu')(decoder_input)\n",
        "x = Reshape((75, 75, 64))(x)\n",
        "x = Conv2DTranspose(64, (3,3), activation='relu', strides=2, padding='same')(x)\n",
        "x = Conv2DTranspose(32, (3,3), activation='relu', strides=2, padding='same')(x)\n",
        "x = Conv2DTranspose(3, (3,3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "outputs = Lambda(lambda img: tf.image.resize(img, (IMG_SIZE, IMG_SIZE)))(x)\n",
        "\n",
        "decoder = Model(decoder_input, outputs, name=\"decoder\")\n",
        "\n",
        "# 📌 VAE Loss with Corrected call() and compile() method\n",
        "class VAE(Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(name=\"reconstruction_loss\")\n",
        "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        if isinstance(data, tuple):\n",
        "            data = data[0]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.keras.losses.binary_crossentropy(\n",
        "                    tf.keras.backend.flatten(data),\n",
        "                    tf.keras.backend.flatten(reconstruction)\n",
        "                )\n",
        "            )\n",
        "\n",
        "            reconstruction_loss *= IMG_SIZE * IMG_SIZE * 3\n",
        "            kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
        "            kl_loss = tf.reduce_mean(kl_loss)\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "    def test_step(self, data):\n",
        "        if isinstance(data, tuple):\n",
        "            data = data[0]\n",
        "\n",
        "        z_mean, z_log_var, z = self.encoder(data)\n",
        "        reconstruction = self.decoder(z)\n",
        "        reconstruction_loss = tf.reduce_mean(\n",
        "            tf.keras.losses.binary_crossentropy(\n",
        "                tf.keras.backend.flatten(data),\n",
        "                tf.keras.backend.flatten(reconstruction)\n",
        "            )\n",
        "        )\n",
        "        reconstruction_loss *= IMG_SIZE * IMG_SIZE * 3\n",
        "        kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
        "        kl_loss = tf.reduce_mean(kl_loss)\n",
        "        total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "# Compile the VAE model\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=Adam())\n"
      ],
      "metadata": {
        "id": "HA3G6kY4mL5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 📌 Train VAE\n",
        "vae.fit(train_normal, train_normal, epochs=30, batch_size=32, validation_split=0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du87h4oLmN25",
        "outputId": "71de9b8b-5dbb-427f-bd1f-9c1040b304be",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 2s/step - kl_loss: 168936768.0000 - loss: -40275648.0000 - reconstruction_loss: -209212400.0000 - val_kl_loss: 15693.8174 - val_loss: -488573600.0000 - val_reconstruction_loss: -488589312.0000\n",
            "Epoch 2/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - kl_loss: 11654.6836 - loss: -469062240.0000 - reconstruction_loss: -469073888.0000 - val_kl_loss: 3883.4180 - val_loss: -488585376.0000 - val_reconstruction_loss: -488589248.0000\n",
            "Epoch 3/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - kl_loss: 3108.8169 - loss: -466785152.0000 - reconstruction_loss: -466788256.0000 - val_kl_loss: 4317.8779 - val_loss: -488584896.0000 - val_reconstruction_loss: -488589312.0000\n",
            "Epoch 4/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - kl_loss: 3702.4260 - loss: -466128224.0000 - reconstruction_loss: -466131872.0000 - val_kl_loss: 1166.4163 - val_loss: -488587872.0000 - val_reconstruction_loss: -488588992.0000\n",
            "Epoch 5/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - kl_loss: 1211.2344 - loss: -465963424.0000 - reconstruction_loss: -465964640.0000 - val_kl_loss: 1512.7745 - val_loss: -488587712.0000 - val_reconstruction_loss: -488589248.0000\n",
            "Epoch 6/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - kl_loss: 1172.0546 - loss: -472362656.0000 - reconstruction_loss: -472363872.0000 - val_kl_loss: 793.6857 - val_loss: -488588352.0000 - val_reconstruction_loss: -488589152.0000\n",
            "Epoch 7/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - kl_loss: 748.9094 - loss: -469812672.0000 - reconstruction_loss: -469813376.0000 - val_kl_loss: 730.4094 - val_loss: -488588480.0000 - val_reconstruction_loss: -488589216.0000\n",
            "Epoch 8/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - kl_loss: 766.9376 - loss: -470358688.0000 - reconstruction_loss: -470359424.0000 - val_kl_loss: 524.3581 - val_loss: -488588608.0000 - val_reconstruction_loss: -488589152.0000\n",
            "Epoch 9/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - kl_loss: 634.3662 - loss: -473055776.0000 - reconstruction_loss: -473056320.0000 - val_kl_loss: 565.6472 - val_loss: -488588640.0000 - val_reconstruction_loss: -488589216.0000\n",
            "Epoch 10/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - kl_loss: 582.2677 - loss: -472067456.0000 - reconstruction_loss: -472068064.0000 - val_kl_loss: 2193.5391 - val_loss: -488587072.0000 - val_reconstruction_loss: -488589312.0000\n",
            "Epoch 11/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - kl_loss: 2606.2739 - loss: -469438272.0000 - reconstruction_loss: -469440896.0000 - val_kl_loss: 614.1262 - val_loss: -488588640.0000 - val_reconstruction_loss: -488589312.0000\n",
            "Epoch 12/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - kl_loss: 518.4030 - loss: -468456864.0000 - reconstruction_loss: -468457344.0000 - val_kl_loss: 532.7490 - val_loss: -488588736.0000 - val_reconstruction_loss: -488589312.0000\n",
            "Epoch 13/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - kl_loss: 488.0150 - loss: -472438144.0000 - reconstruction_loss: -472438592.0000 - val_kl_loss: 341.1534 - val_loss: -488588640.0000 - val_reconstruction_loss: -488588992.0000\n",
            "Epoch 14/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - kl_loss: 473.5219 - loss: -466812704.0000 - reconstruction_loss: -466813152.0000 - val_kl_loss: 411.0474 - val_loss: -488588864.0000 - val_reconstruction_loss: -488589312.0000\n",
            "Epoch 15/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - kl_loss: 440.4537 - loss: -470145696.0000 - reconstruction_loss: -470146176.0000 - val_kl_loss: 357.3727 - val_loss: -488588896.0000 - val_reconstruction_loss: -488589312.0000\n",
            "Epoch 16/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - kl_loss: 355.0771 - loss: -469283808.0000 - reconstruction_loss: -469284224.0000 - val_kl_loss: 313.1257 - val_loss: -488588960.0000 - val_reconstruction_loss: -488589312.0000\n",
            "Epoch 17/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - kl_loss: 388.5905 - loss: -470248256.0000 - reconstruction_loss: -470248704.0000 - val_kl_loss: 295.5960 - val_loss: -488588896.0000 - val_reconstruction_loss: -488589216.0000\n",
            "Epoch 18/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - kl_loss: 880.1547 - loss: -472905696.0000 - reconstruction_loss: -472906592.0000 - val_kl_loss: 437.3512 - val_loss: -488588864.0000 - val_reconstruction_loss: -488589312.0000\n",
            "Epoch 19/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - kl_loss: 589.1389 - loss: -475115616.0000 - reconstruction_loss: -475116160.0000 - val_kl_loss: 516.2227 - val_loss: -488588736.0000 - val_reconstruction_loss: -488589312.0000\n",
            "Epoch 20/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - kl_loss: 410.9369 - loss: -467185056.0000 - reconstruction_loss: -467185408.0000 - val_kl_loss: 667.7327 - val_loss: -488588608.0000 - val_reconstruction_loss: -488589312.0000\n",
            "Epoch 21/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - kl_loss: 609.2139 - loss: -467305728.0000 - reconstruction_loss: -467306240.0000 - val_kl_loss: 805.9601 - val_loss: -488588448.0000 - val_reconstruction_loss: -488589312.0000\n",
            "Epoch 22/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - kl_loss: 616.7031 - loss: -466476416.0000 - reconstruction_loss: -466477024.0000 - val_kl_loss: 366.1392 - val_loss: -488588896.0000 - val_reconstruction_loss: -488589312.0000\n",
            "Epoch 23/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - kl_loss: 428.3418 - loss: -465201664.0000 - reconstruction_loss: -465202112.0000 - val_kl_loss: 469.4200 - val_loss: -488588800.0000 - val_reconstruction_loss: -488589312.0000\n",
            "Epoch 24/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - kl_loss: 370.9403 - loss: -467052960.0000 - reconstruction_loss: -467053280.0000 - val_kl_loss: 301.8673 - val_loss: -488588960.0000 - val_reconstruction_loss: -488589312.0000\n",
            "Epoch 25/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - kl_loss: 331.6349 - loss: -470777024.0000 - reconstruction_loss: -470777312.0000 - val_kl_loss: 278.8543 - val_loss: -488588960.0000 - val_reconstruction_loss: -488589312.0000\n",
            "Epoch 26/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - kl_loss: 362.9439 - loss: -472944768.0000 - reconstruction_loss: -472945088.0000 - val_kl_loss: 379.0593 - val_loss: -488588896.0000 - val_reconstruction_loss: -488589312.0000\n",
            "Epoch 27/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - kl_loss: 692.7703 - loss: -470453888.0000 - reconstruction_loss: -470454560.0000 - val_kl_loss: 335.3148 - val_loss: -488588960.0000 - val_reconstruction_loss: -488589312.0000\n",
            "Epoch 28/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - kl_loss: 370.8779 - loss: -470951328.0000 - reconstruction_loss: -470951680.0000 - val_kl_loss: 260.0529 - val_loss: -488588960.0000 - val_reconstruction_loss: -488589216.0000\n",
            "Epoch 29/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - kl_loss: 733.4068 - loss: -471818752.0000 - reconstruction_loss: -471819424.0000 - val_kl_loss: 1573.8505 - val_loss: -488587712.0000 - val_reconstruction_loss: -488589312.0000\n",
            "Epoch 30/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - kl_loss: 1702.2632 - loss: -468171968.0000 - reconstruction_loss: -468173664.0000 - val_kl_loss: 1242.3182 - val_loss: -488588032.0000 - val_reconstruction_loss: -488589312.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79c962125310>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 📌 Anomaly Detection with VAE\n",
        "z_mean_normal, _, _ = encoder.predict(test_normal)\n",
        "z_mean_pneumonia, _, _ = encoder.predict(test_pneumonia)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFuLrJOF1l25",
        "outputId": "9968e19b-ad3b-459c-e3c9-ba0b9ea00735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 4s/step\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Mahalanobis distance\n",
        "ec = EmpiricalCovariance()\n",
        "ec.fit(z_mean_normal)\n",
        "\n",
        "mahalanobis_normal = ec.mahalanobis(z_mean_normal)\n",
        "mahalanobis_pneumonia = ec.mahalanobis(z_mean_pneumonia)"
      ],
      "metadata": {
        "id": "-2nL3lho1oze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "y_true = np.concatenate([np.zeros(len(mahalanobis_normal)), np.ones(len(mahalanobis_pneumonia))])\n",
        "y_scores = np.concatenate([mahalanobis_normal, mahalanobis_pneumonia])\n",
        "threshold = np.percentile(mahalanobis_normal, 95)\n",
        "y_pred = (y_scores > threshold).astype(int)"
      ],
      "metadata": {
        "id": "y3R7ozj81s5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
        "\n",
        "# 📌 Evaluate Metrics\n",
        "auc_roc = roc_auc_score(y_true, y_scores)  # AUC-ROC\n",
        "auc_pr = average_precision_score(y_true, y_scores)  # AUC-PR\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "# 📌 Print Results\n",
        "print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
        "print(f\"AUC-PR: {auc_pr:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ24CrykDQo0",
        "outputId": "be6e6fbc-8c90-4281-bc99-852ba8011ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC-ROC: 0.9231\n",
            "AUC-PR: 0.9567\n",
            "Accuracy: 0.8093\n",
            "Precision: 0.9593\n",
            "Recall: 0.7256\n",
            "F1 Score: 0.8263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6BxqKeV8EM02"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}